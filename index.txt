1:"$Sreact.fragment"
2:I[7558,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"ThemeProvider"]
3:I[9994,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
b:I[7150,[],""]
:HL["/_next/static/css/19bf7477d82f620e.css","style"]
0:{"P":null,"b":"2zdQR6yi6BYZ4pCI9QyDc","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/19bf7477d82f620e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Xiaolin Chen","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":["__PAGE__","$L9",{},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[7923,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"default"]
d:I[9756,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-31918c9c1b40d633.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-170cc7b5e252a482.js","974","static/chunks/app/page-066fdedc098f144b.js"],"default"]
e:I[470,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-31918c9c1b40d633.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-170cc7b5e252a482.js","974","static/chunks/app/page-066fdedc098f144b.js"],"default"]
f:I[2597,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-31918c9c1b40d633.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-170cc7b5e252a482.js","974","static/chunks/app/page-066fdedc098f144b.js"],"default"]
1a:I[4431,[],"ViewportBoundary"]
1c:I[4431,[],"MetadataBoundary"]
1d:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Lc",null,{"lastUpdated":"November 18, 2025"}]
10:T5bc,Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type’s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.11:T6ec,@article{chen2025dual,
  title = {Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems},
  author = {Chen, Xiaolin and Song, Xuemeng and Wen, Haokun and Guan, Weili and Zhao, Xiangyu and Nie, Liqiang},
  journal = {arXiv preprint arXiv:2509.07817},
  year = {2025},
  abstract = {Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type’s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately
 summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.}
}12:T629,Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.13:T7cf,@article{10.1145/3704811,
  author = {Chen, Xiaolin and Song, Xuemeng and Zuo, Jianhui and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  title = {Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling},
  year = {2024},
  volume = {21},
  number = {2},
  pages = {1--22},
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  abstract = {Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.}
}14:T5e5,Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.15:T777,@article{10.1145/3606368,
  author = {Chen, Xiaolin and Song, Xuemeng and Jing, Liqiang and Li, Shuo and Hu, Linmei and Nie, Liqiang},
  title = {Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model},
  year = {2023},
  publisher = {ACM},
  volume = {42},
  number = {2},
  journal = {ACM Transactions on Information Systems},
  pages = {1--25},
  abstract = {Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.}
}16:T5e8,Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.17:T780,@inproceedings{10.1145/3539618.3591673,
  author = {Chen, Xiaolin and Song, Xuemeng and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  title = {Dual Semantic Knowledge Composed Multimodal Dialog Systems},
  year = {2023},
  publisher = {ACM},
  booktitle = {Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {1--25},
  abstract = {Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.}
}9:["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$Ld",null,{"author":{"name":"Xiaolin Chen","title":"Postdoctoral Research Fellow","institution":"National University of Singapore (NUS)","avatar":"/xiaolin2.jpg"},"social":{"email":"cxlicd@gmail.com","location":"Singapore","location_url":"https://maps.google.com","location_details":["innovation 4.0, #04-06","3 Research Link, Singapore 117602"],"google_scholar":"https://scholar.google.com/citations?user=3g0B6Q0AAAAJ&hl=en","orcid":"https://orcid.org/my-orcid?orcid=0000-0003-4638-0603","github":"https://github.com/XiaolinChen-XL/"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Multimedia Computing","Information Retrieval"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$Le","about",{"content":"My name is Xiaolin Chen (陈潇琳). I am currently a Postdoctoral Research Fellow at the [National University of Singapore (NUS)](https://nus.edu.sg/), under the supervision of [Prof. Wynne Hsu](https://www.comp.nus.edu.sg/~whsu/) and  [Prof. Mong Li Lee](https://www.comp.nus.edu.sg/~leeml/). I received my Ph.D. degree at [Shandong University](https://www.sdu.edu.cn/), under the supervision of [Prof. Liqiang Nie](https://liqiangnie.github.io/index.html) and [Prof. Xuemeng Song](https://xuemengsong.github.io/). From November 2022 to December 2023, I was a visiting Ph.D. student at [NExT++](https://www.nextcenter.org/) Lab, NUS, supervised by [Prof. Tat-Seng Chua](https://www.comp.nus.edu.sg/cs/people/chuats/) and [Prof. Yinwei Wei](https://weiyinwei.github.io/).","title":"About"}],["$","$Lf","featured_publications",{"publications":[{"id":"chen2025dual","title":"Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haokun Wen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Weili Guan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangyu Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2509.07817","conference":"","abstract":"$10","description":"","selected":true,"preview":"2025_DK2R.jpg","bibtex":"$11"},{"id":"10.1145/3704811","title":"Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jianhui Zuo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tat-Seng Chua","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"ACM Transactions on Multimedia Computing, Communications, and Applications","conference":"","volume":"21","issue":"2","pages":"1--22","code":"https://multimodaldialogs.wixstudio.com/dmdu","abstract":"$12","description":"","selected":true,"preview":"2924_DMDU.png","bibtex":"$13"},{"id":"10.1145/3606368","title":"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Jing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shuo Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linmei Hu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems","conference":"","volume":"42","issue":"2","pages":"1--25","code":"https://multimodaldialog.wixsite.com/website","abstract":"$14","description":"","selected":true,"preview":"2023_DKMD.png","bibtex":"$15"},{"id":"10.1145/3539618.3591673","title":"Dual Semantic Knowledge Composed Multimodal Dialog Systems","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tat-Seng Chua","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval","pages":"1--25","code":"https://dialogdk2r.wixsite.com/anonymous7357","abstract":"$16","description":"","selected":true,"preview":"2023_MDS.png","bibtex":"$17"}],"title":"Selected Publications","enableOnePageMode":true}],"$L18"],false,false,false]}]]}]]}]}],null,"$L19"]}]
a:["$","$1","h",{"children":[null,[["$","$L1a",null,{"children":"$L1b"}],null],["$","$L1c",null,{"children":["$","div",null,{"hidden":true,"children":["$","$1d",null,{"fallback":null,"children":"$L1e"}]}]}]]}]
1f:I[5078,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-31918c9c1b40d633.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-170cc7b5e252a482.js","974","static/chunks/app/page-066fdedc098f144b.js"],"default"]
20:I[4431,[],"OutletBoundary"]
22:I[5278,[],"AsyncMetadataOutlet"]
18:["$","$L1f","news",{"items":[{"date":"2025-10","content":"Starting my Postdoc at National University of Singapore"}],"title":"News"}]
19:["$","$L20",null,{"children":["$L21",["$","$L22",null,{"promise":"$@23"}]]}]
1b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
21:null
24:I[622,[],"IconMark"]
23:{"metadata":[["$","title","0",{"children":"Xiaolin Chen"}],["$","meta","1",{"name":"description","content":"Homepage of Xiaolin"}],["$","meta","2",{"name":"author","content":"Xiaolin Chen"}],["$","meta","3",{"name":"keywords","content":"Xiaolin Chen,PhD,Research,National University of Singapore (NUS)"}],["$","meta","4",{"name":"creator","content":"Xiaolin Chen"}],["$","meta","5",{"name":"publisher","content":"Xiaolin Chen"}],["$","meta","6",{"property":"og:title","content":"Xiaolin Chen"}],["$","meta","7",{"property":"og:description","content":"Homepage of Xiaolin"}],["$","meta","8",{"property":"og:site_name","content":"Xiaolin Chen's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Xiaolin Chen"}],["$","meta","13",{"name":"twitter:description","content":"Homepage of Xiaolin"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L24","15",{}]],"error":null,"digest":"$undefined"}
1e:"$23:metadata"
