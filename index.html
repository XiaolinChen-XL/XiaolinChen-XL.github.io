<!DOCTYPE html><!--2zdQR6yi6BYZ4pCI9QyDc--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/xiaolin2.jpg"/><link rel="stylesheet" href="/_next/static/css/19bf7477d82f620e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-4a6fea9592d99ef7.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-0fb808ac7cfa018d.js" async=""></script><script src="/_next/static/chunks/main-app-fa86e377ec922326.js" async=""></script><script src="/_next/static/chunks/185-02e5984e85404956.js" async=""></script><script src="/_next/static/chunks/619-9168df9c2a29b74b.js" async=""></script><script src="/_next/static/chunks/599-36b43173d762fc47.js" async=""></script><script src="/_next/static/chunks/app/layout-4c16ce53a11bdf11.js" async=""></script><script src="/_next/static/chunks/140-31918c9c1b40d633.js" async=""></script><script src="/_next/static/chunks/681-170cc7b5e252a482.js" async=""></script><script src="/_next/static/chunks/app/page-066fdedc098f144b.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><title>Xiaolin Chen</title><meta name="description" content="Homepage of Xiaolin"/><meta name="author" content="Xiaolin Chen"/><meta name="keywords" content="Xiaolin Chen,PhD,Research,National University of Singapore (NUS)"/><meta name="creator" content="Xiaolin Chen"/><meta name="publisher" content="Xiaolin Chen"/><meta property="og:title" content="Xiaolin Chen"/><meta property="og:description" content="Homepage of Xiaolin"/><meta property="og:site_name" content="Xiaolin Chen&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Xiaolin Chen"/><meta name="twitter:description" content="Homepage of Xiaolin"/><link rel="icon" href="/favicon.svg"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Xiaolin Chen</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_5pdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Xiaolin Chen" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/xiaolin2.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Xiaolin Chen</h1><p class="text-lg text-accent font-medium mb-1">Postdoctoral Research Fellow</p><p class="text-neutral-600 mb-2">National University of Singapore (NUS)</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=3g0B6Q0AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/my-orcid?orcid=0000-0003-4638-0603" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/XiaolinChen-XL/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Multimedia Computing</div><div>Information Retrieval</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">My name is Xiaolin Chen (ÈôàÊΩáÁê≥). I am currently a Postdoctoral Research Fellow at the <a href="https://nus.edu.sg/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">National University of Singapore (NUS)</a>, under the supervision of <a href="https://www.comp.nus.edu.sg/~whsu/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Wynne Hsu</a> and  <a href="https://www.comp.nus.edu.sg/~leeml/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Mong Li Lee</a>. I received my Ph.D. degree at <a href="https://www.sdu.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Shandong University</a>, under the supervision of <a href="https://liqiangnie.github.io/index.html" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Liqiang Nie</a> and <a href="https://xuemengsong.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Xuemeng Song</a>. From November 2022 to December 2023, I was a visiting Ph.D. student at <a href="https://www.nextcenter.org/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">NExT++</a> Lab, NUS, supervised by <a href="https://www.comp.nus.edu.sg/cs/people/chuats/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Tat-Seng Chua</a> and <a href="https://weiyinwei.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Yinwei Wei</a>.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/#publications">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Xiaolin Chen</span>, </span><span><span class="">Xuemeng Song</span>, </span><span><span class="">Haokun Wen</span>, </span><span><span class="">Weili Guan</span>, </span><span><span class="">Xiangyu Zhao</span>, </span><span><span class="">Liqiang Nie</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">arXiv preprint arXiv:2509.07817</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Xiaolin Chen</span>, </span><span><span class="">Xuemeng Song</span>, </span><span><span class="">Jianhui Zuo</span>, </span><span><span class="">Yinwei Wei</span>, </span><span><span class="">Liqiang Nie</span>, </span><span><span class="">Tat-Seng Chua</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Multimedia Computing, Communications, and Applications</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Xiaolin Chen</span>, </span><span><span class="">Xuemeng Song</span>, </span><span><span class="">Liqiang Jing</span>, </span><span><span class="">Shuo Li</span>, </span><span><span class="">Linmei Hu</span>, </span><span><span class="">Liqiang Nie</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Information Systems</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Dual Semantic Knowledge Composed Multimodal Dialog Systems</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Xiaolin Chen</span>, </span><span><span class="">Xuemeng Song</span>, </span><span><span class="">Yinwei Wei</span>, </span><span><span class="">Liqiang Nie</span>, </span><span><span class="">Tat-Seng Chua</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-10</span><p class="text-sm text-neutral-700">Starting my Postdoc at National University of Singapore</p></div></div></section></section></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 18, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-4a6fea9592d99ef7.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7558,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-4c16ce53a11bdf11.js\"],\"ThemeProvider\"]\n3:I[9994,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-4c16ce53a11bdf11.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\nb:I[7150,[],\"\"]\n:HL[\"/_next/static/css/19bf7477d82f620e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"2zdQR6yi6BYZ4pCI9QyDc\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19bf7477d82f620e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Xiaolin Chen\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],\"$L6\",\"$L7\"]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],\"$L8\"]}]}]]}]]}],{\"children\":[\"__PAGE__\",\"$L9\",{},null,false]},null,false],\"$La\",false]],\"m\":\"$undefined\",\"G\":[\"$b\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:I[7923,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-4c16ce53a11bdf11.js\"],\"default\"]\nd:I[9756,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-31918c9c1b40d633.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-170cc7b5e252a482.js\",\"974\",\"static/chunks/app/page-066fdedc098f144b.js\"],\"default\"]\ne:I[470,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-31918c9c1b40d633.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-170cc7b5e252a482.js\",\"974\",\"static/chunks/app/page-066fdedc098f144b.js\"],\"default\"]\nf:I[2597,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-31918c9c1b40d633.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-170cc7b5e252a482.js\",\"974\",\"static/chunks/app/page-066fdedc098f144b.js\"],\"default\"]\n1a:I[4431,[],\"ViewportBoundary\"]\n1c:I[4431,[],\"MetadataBoundary\"]\n1d:\"$Sreact.suspense\"\n6:[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}]\n7:[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]\n8:[\"$\",\"$Lc\",null,{\"lastUpdated\":\"November 18, 2025\"}]\n10:T5bc,"])</script><script>self.__next_f.push([1,"Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type‚Äôs utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters."])</script><script>self.__next_f.push([1,"11:T6ec,"])</script><script>self.__next_f.push([1,"@article{chen2025dual,\n  title = {Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems},\n  author = {Chen, Xiaolin and Song, Xuemeng and Wen, Haokun and Guan, Weili and Zhao, Xiangyu and Nie, Liqiang},\n  journal = {arXiv preprint arXiv:2509.07817},\n  year = {2025},\n  abstract = {Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type‚Äôs utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately\n summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.}\n}"])</script><script>self.__next_f.push([1,"12:T629,"])</script><script>self.__next_f.push([1,"Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers."])</script><script>self.__next_f.push([1,"13:T7cf,"])</script><script>self.__next_f.push([1,"@article{10.1145/3704811,\n  author = {Chen, Xiaolin and Song, Xuemeng and Zuo, Jianhui and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},\n  title = {Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling},\n  year = {2024},\n  volume = {21},\n  number = {2},\n  pages = {1--22},\n  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},\n  abstract = {Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.}\n}"])</script><script>self.__next_f.push([1,"14:T5e5,"])</script><script>self.__next_f.push([1,"Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors."])</script><script>self.__next_f.push([1,"15:T777,"])</script><script>self.__next_f.push([1,"@article{10.1145/3606368,\n  author = {Chen, Xiaolin and Song, Xuemeng and Jing, Liqiang and Li, Shuo and Hu, Linmei and Nie, Liqiang},\n  title = {Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model},\n  year = {2023},\n  publisher = {ACM},\n  volume = {42},\n  number = {2},\n  journal = {ACM Transactions on Information Systems},\n  pages = {1--25},\n  abstract = {Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.}\n}"])</script><script>self.__next_f.push([1,"16:T5e8,"])</script><script>self.__next_f.push([1,"Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community."])</script><script>self.__next_f.push([1,"17:T780,"])</script><script>self.__next_f.push([1,"@inproceedings{10.1145/3539618.3591673,\n  author = {Chen, Xiaolin and Song, Xuemeng and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},\n  title = {Dual Semantic Knowledge Composed Multimodal Dialog Systems},\n  year = {2023},\n  publisher = {ACM},\n  booktitle = {Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  pages = {1--25},\n  abstract = {Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.}\n}"])</script><script>self.__next_f.push([1,"9:[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$Ld\",null,{\"author\":{\"name\":\"Xiaolin Chen\",\"title\":\"Postdoctoral Research Fellow\",\"institution\":\"National University of Singapore (NUS)\",\"avatar\":\"/xiaolin2.jpg\"},\"social\":{\"email\":\"cxlicd@gmail.com\",\"location\":\"Singapore\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"innovation 4.0, #04-06\",\"3 Research Link, Singapore 117602\"],\"google_scholar\":\"https://scholar.google.com/citations?user=3g0B6Q0AAAAJ\u0026hl=en\",\"orcid\":\"https://orcid.org/my-orcid?orcid=0000-0003-4638-0603\",\"github\":\"https://github.com/XiaolinChen-XL/\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Multimedia Computing\",\"Information Retrieval\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$Le\",\"about\",{\"content\":\"My name is Xiaolin Chen (ÈôàÊΩáÁê≥). I am currently a Postdoctoral Research Fellow at the [National University of Singapore (NUS)](https://nus.edu.sg/), under the supervision of [Prof. Wynne Hsu](https://www.comp.nus.edu.sg/~whsu/) and  [Prof. Mong Li Lee](https://www.comp.nus.edu.sg/~leeml/). I received my Ph.D. degree at [Shandong University](https://www.sdu.edu.cn/), under the supervision of [Prof. Liqiang Nie](https://liqiangnie.github.io/index.html) and [Prof. Xuemeng Song](https://xuemengsong.github.io/). From November 2022 to December 2023, I was a visiting Ph.D. student at [NExT++](https://www.nextcenter.org/) Lab, NUS, supervised by [Prof. Tat-Seng Chua](https://www.comp.nus.edu.sg/cs/people/chuats/) and [Prof. Yinwei Wei](https://weiyinwei.github.io/).\",\"title\":\"About\"}],[\"$\",\"$Lf\",\"featured_publications\",{\"publications\":[{\"id\":\"chen2025dual\",\"title\":\"Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems\",\"authors\":[{\"name\":\"Xiaolin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuemeng Song\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haokun Wen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Weili Guan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Liqiang Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2509.07817\",\"conference\":\"\",\"abstract\":\"$10\",\"description\":\"\",\"selected\":true,\"preview\":\"2025_DK2R.jpg\",\"bibtex\":\"$11\"},{\"id\":\"10.1145/3704811\",\"title\":\"Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling\",\"authors\":[{\"name\":\"Xiaolin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuemeng Song\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianhui Zuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinwei Wei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Liqiang Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tat-Seng Chua\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Multimedia Computing, Communications, and Applications\",\"conference\":\"\",\"volume\":\"21\",\"issue\":\"2\",\"pages\":\"1--22\",\"code\":\"https://multimodaldialogs.wixstudio.com/dmdu\",\"abstract\":\"$12\",\"description\":\"\",\"selected\":true,\"preview\":\"2924_DMDU.png\",\"bibtex\":\"$13\"},{\"id\":\"10.1145/3606368\",\"title\":\"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model\",\"authors\":[{\"name\":\"Xiaolin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuemeng Song\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Liqiang Jing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuo Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linmei Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Liqiang Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Information Systems\",\"conference\":\"\",\"volume\":\"42\",\"issue\":\"2\",\"pages\":\"1--25\",\"code\":\"https://multimodaldialog.wixsite.com/website\",\"abstract\":\"$14\",\"description\":\"\",\"selected\":true,\"preview\":\"2023_DKMD.png\",\"bibtex\":\"$15\"},{\"id\":\"10.1145/3539618.3591673\",\"title\":\"Dual Semantic Knowledge Composed Multimodal Dialog Systems\",\"authors\":[{\"name\":\"Xiaolin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuemeng Song\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinwei Wei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Liqiang Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tat-Seng Chua\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval\",\"pages\":\"1--25\",\"code\":\"https://dialogdk2r.wixsite.com/anonymous7357\",\"abstract\":\"$16\",\"description\":\"\",\"selected\":true,\"preview\":\"2023_MDS.png\",\"bibtex\":\"$17\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":true}],\"$L18\"],false,false,false]}]]}]]}]}],null,\"$L19\"]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L1a\",null,{\"children\":\"$L1b\"}],null],[\"$\",\"$L1c\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$1d\",null,{\"fallback\":null,\"children\":\"$L1e\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"1f:I[5078,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-31918c9c1b40d633.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-170cc7b5e252a482.js\",\"974\",\"static/chunks/app/page-066fdedc098f144b.js\"],\"default\"]\n20:I[4431,[],\"OutletBoundary\"]\n22:I[5278,[],\"AsyncMetadataOutlet\"]\n18:[\"$\",\"$L1f\",\"news\",{\"items\":[{\"date\":\"2025-10\",\"content\":\"Starting my Postdoc at National University of Singapore\"}],\"title\":\"News\"}]\n19:[\"$\",\"$L20\",null,{\"children\":[\"$L21\",[\"$\",\"$L22\",null,{\"promise\":\"$@23\"}]]}]\n"])</script><script>self.__next_f.push([1,"1b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"21:null\n"])</script><script>self.__next_f.push([1,"24:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"23:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Homepage of Xiaolin\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Xiaolin Chen,PhD,Research,National University of Singapore (NUS)\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Homepage of Xiaolin\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Xiaolin Chen's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Xiaolin Chen\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Homepage of Xiaolin\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L24\",\"15\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"1e:\"$23:metadata\"\n"])</script></body></html>