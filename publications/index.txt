1:"$Sreact.fragment"
2:I[7558,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"ThemeProvider"]
3:I[9994,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
c:I[7150,[],""]
:HL["/_next/static/css/19bf7477d82f620e.css","style"]
0:{"P":null,"b":"2zdQR6yi6BYZ4pCI9QyDc","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/19bf7477d82f620e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Xiaolin Chen","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":[["slug","publications","d"],"$L9",{"children":["__PAGE__","$La",{},null,false]},null,false]},null,false],"$Lb",false]],"m":"$undefined","G":["$c",[]],"s":false,"S":true}
d:I[7923,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-4c16ce53a11bdf11.js"],"default"]
f:I[4431,[],"OutletBoundary"]
11:I[5278,[],"AsyncMetadataOutlet"]
13:I[4431,[],"ViewportBoundary"]
15:I[4431,[],"MetadataBoundary"]
16:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Ld",null,{"lastUpdated":"November 18, 2025"}]
9:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
a:["$","$1","c",{"children":["$Le",null,["$","$Lf",null,{"children":["$L10",["$","$L11",null,{"promise":"$@12"}]]}]]}]
b:["$","$1","h",{"children":[null,[["$","$L13",null,{"children":"$L14"}],null],["$","$L15",null,{"children":["$","div",null,{"hidden":true,"children":["$","$16",null,{"fallback":null,"children":"$L17"}]}]}]]}]
18:I[9958,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-31918c9c1b40d633.js","681","static/chunks/681-170cc7b5e252a482.js","182","static/chunks/app/%5Bslug%5D/page-244b71ef69fa9e38.js"],"default"]
19:T5bc,Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type’s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.1a:T6ec,@article{chen2025dual,
  title = {Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems},
  author = {Chen, Xiaolin and Song, Xuemeng and Wen, Haokun and Guan, Weili and Zhao, Xiangyu and Nie, Liqiang},
  journal = {arXiv preprint arXiv:2509.07817},
  year = {2025},
  abstract = {Textual response generation is pivotal for multimodal task-oriented dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) neglect of unstructured review knowledge and 2) underutilization of large language models (LLMs). Inspired by this, we aim to fully utilize dual knowledge (i.e., structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) dynamic knowledge type selection and 2) intention-response decoupling. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type’s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately
 summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.}
}1b:T583,@article{becattini2024interactive,
  title = {Interactive Garment Recommendation with User in the Loop},
  author = {Becattini, Federico and Chen, Xiaolin and Puccia, Andrea and Wen, Haokun and Song, Xuemeng and Nie, Liqiang and Del Bimbo, Alberto},
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  volume = {21},
  number = {1},
  pages = {37:1--37:21},
  year = {2025},
  abstract = {Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement
 learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.}
}1c:T629,Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.1d:T7cf,@article{10.1145/3704811,
  author = {Chen, Xiaolin and Song, Xuemeng and Zuo, Jianhui and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  title = {Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling},
  year = {2024},
  volume = {21},
  number = {2},
  pages = {1--22},
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  abstract = {Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction, domain-aware response generation, and distribution-based user characteristic injection. Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.}
}1e:T68b,Composed image retrieval (CIR) aims to retrieve the target image based on a multimodal query, i.e., a reference image paired with corresponding modification text. Recent CIR studies leverage vision-language pre-trained (VLP) methods as the feature extraction backbone and perform nonlinear feature-level multimodal query fusion to retrieve the target image. Despite the promising performance, we argue that their nonlinear feature-level multimodal fusion may lead to the fused feature deviating from the original embedding space, potentially hurting the retrieval performance. To address this issue, in this work, we propose shifting the multimodal fusion from the feature level to the raw-data level to fully exploit the VLP model's multimodal encoding and cross-modal alignment abilities. In particular, we introduce a Dual Query Unification-based Composed Image Retrieval framework (DQU-CIR), whose backbone simply involves a VLP model's image encoder and a text encoder. Specifically, DQU-CIR first employs two training-free query unification components to derive a unified textual and visual query based on the raw data of the multimodal query, respectively. The unified textual query is derived by concatenating the modification text with the extracted reference image's textual description, while the unified visual query is created by writing the key modification words onto the reference image. Ultimately, to address diverse search intentions, DQU-CIR linearly combines the features of the two unified queries encoded by the VLP model to retrieve the target image. Extensive experiments on four real-world datasets validate the effectiveness of our proposed method.1f:T856,@inproceedings{wen2024simple,
  title = {Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval},
  author = {Wen, Haokun and Song, Xuemeng and Chen, Xiaolin and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  booktitle = {Proceedings of the International ACM SIGIR Conference on
                  Research and Development in Information Retrieval},
  pages = {229--239},
  publisher = {ACM},
  year = {2024},
  abstract = {Composed image retrieval (CIR) aims to retrieve the target image based on a multimodal query, i.e., a reference image paired with corresponding modification text. Recent CIR studies leverage vision-language pre-trained (VLP) methods as the feature extraction backbone and perform nonlinear feature-level multimodal query fusion to retrieve the target image. Despite the promising performance, we argue that their nonlinear feature-level multimodal fusion may lead to the fused feature deviating from the original embedding space, potentially hurting the retrieval performance. To address this issue, in this work, we propose shifting the multimodal fusion from the feature level to the raw-data level to fully exploit the VLP model's multimodal encoding and cross-modal alignment abilities. In particular, we introduce a Dual Query Unification-based Composed Image Retrieval framework (DQU-CIR), whose backbone simply involves a VLP model's image encoder and a text encoder. Specifically, DQU-CIR first employs two training-free query unification components to derive a unified textual and visual query based on the raw data of the multimodal query, respectively. The unified textual query is derived by concatenating the modification text with the extracted reference image's textual description, while the unified visual query is created by writing the key modification words onto the reference image. Ultimately, to address diverse search intentions, DQU-CIR linearly combines the features of the two unified queries encoded by the VLP model to retrieve the target image. Extensive experiments on four real-world datasets validate the effectiveness of our proposed method.}
}20:T5e5,Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.21:T777,@article{10.1145/3606368,
  author = {Chen, Xiaolin and Song, Xuemeng and Jing, Liqiang and Li, Shuo and Hu, Linmei and Nie, Liqiang},
  title = {Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model},
  year = {2023},
  publisher = {ACM},
  volume = {42},
  number = {2},
  journal = {ACM Transactions on Information Systems},
  pages = {1--25},
  abstract = {Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.}
}22:T5e8,Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.23:T780,@inproceedings{10.1145/3539618.3591673,
  author = {Chen, Xiaolin and Song, Xuemeng and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  title = {Dual Semantic Knowledge Composed Multimodal Dialog Systems},
  year = {2023},
  publisher = {ACM},
  booktitle = {Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {1--25},
  abstract = {Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.}
}24:T4ba,Visual Commonsense Reasoning (VCR) calls for explanatory reasoning behind question answering over visual scenes. To achieve this goal, a model is required to provide an acceptable rationale as the reason for the predicted answers. Progress on the benchmark dataset stems largely from the recent advancement of Vision Language Transformers (VL Transformers). These models are first pre-trained on some generic large-scale vision-text datasets, and then the learned representations are transferred to the downstream VCR task. Despite their attractive performance, this paper posits that the VLTransformersdonotexhibitvisualcommonsense,which is the key to VCR. In particular, our empirical results pinpoint several shortcomings of existing VL Transformers: small gains from pre-training, unexpected language bias, limited model architecture for the two inseparable sub-tasks, and neglect of the important object-tag correlation. With these findings, we tentatively suggest somefuture directions from the aspect of dataset, evaluation metric, and training tricks. We believe this work could make researchers revisit the intuition and goals of VCR, and thus help tackle the remaining challenges in visual reasoning.25:T616,@article{Li2023DoVT,
  title = {Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR},
  author = {Zhenyang Li and Yangyang Guo and Ke-Jyun Wang and Xiaolin Chen and Liqiang Nie and Mohan Kankanhalli},
  journal = {Proceedings of the ACM International Conference on Multimedia},
  year = {2023},
  abstract = {Visual Commonsense Reasoning (VCR) calls for explanatory reasoning behind question answering over visual scenes. To achieve
 this goal, a model is required to provide an acceptable rationale as the reason for the predicted answers. Progress on the benchmark dataset stems largely from the recent advancement of Vision Language Transformers (VL Transformers). These models are first pre-trained on some generic large-scale vision-text datasets, and then the learned representations are transferred to the downstream VCR task. Despite their attractive performance, this paper posits that the VLTransformersdonotexhibitvisualcommonsense,which is the key to VCR. In particular, our empirical results pinpoint several shortcomings of existing VL Transformers: small gains from pre-training, unexpected language bias, limited model architecture for the two inseparable sub-tasks, and neglect of the important object-tag correlation. With these findings, we tentatively suggest somefuture directions from the aspect of dataset, evaluation metric, and training tricks. We believe this work could make researchers revisit the intuition and goals of VCR, and thus help tackle the
 remaining challenges in visual reasoning.}
}26:T433,Sarcasm is a sophisticated linguistic phenomenon that is prevalent on today's social media platforms. Multi-modal sarcasm detection aims to identify whether a given sample with multi-modal information (i.e., text and image) is sarcastic. This task's key lies in capturing both inter- and intra-modal incongruities within the same context. Although existing methods have achieved compelling success, they are disturbed by irrelevant information extracted from the whole image and text, or overlooking some important information due to the incomplete input. To address these limitations, we propose a Mutual-enhanced Incongruity Learning Network for multi-modal sarcasm detection, named MILNet. In particular, we design a local semantic-guided incongruity learning module and a global incongruity learning module. Moreover, we introduce a mutual enhancement module to take advantage of the underlying consistency between the two modules to boost the performance. Extensive experiments on a widely-used dataset demonstrate the superiority of our model over cutting-edge methods.27:T5b2,@inproceedings{10.1609/aaai.v37i8.26138,
  author = {Qiao, Yang and Jing, Liqiang and Song, Xuemeng and Chen, Xiaolin and Zhu, Lei and Nie, Liqiang},
  title = {Mutual-enhanced incongruity learning network for multi-modal sarcasm detection},
  year = {2023},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  abstract = {Sarcasm is a sophisticated linguistic phenomenon that is prevalent on today's social media platforms. Multi-modal sarcasm detection aims to identify whether a given sample with multi-modal information (i.e., text and image) is sarcastic. This task's key lies in capturing both inter- and intra-modal incongruities within the same context. Although existing methods have achieved compelling success, they are disturbed by irrelevant information extracted from the whole image and text, or overlooking some important information due to the incomplete input. To address these limitations, we propose a Mutual-enhanced Incongruity Learning Network for multi-modal sarcasm detection, named MILNet. In particular, we design a local semantic-guided incongruity learning module and a global incongruity learning module. Moreover, we introduce a mutual enhancement module to take advantage of the underlying consistency between the two modules to boost the performance. Extensive experiments on a widely-used dataset demonstrate the superiority of our model over cutting-edge methods.}
}28:T5b8,Outfit compatibility modeling, which aims to automatically evaluate the matching degree of an outfit, has drawn great research attention. Regarding the comprehensive evaluation, several previous studies have attempted to solve the task of outfit compatibility modeling by integrating the multi-modal information of fashion items. However, these methods primarily focus on fusing the visual and textual modalities, but seldom consider the category modality as an essential modality. In addition, they mainly focus on the exploration of the intra-modal compatibility relation among fashion items in an outfit but ignore the importance of the inter-modal compatibility relation, i.e., the compatibility across different modalities between fashion items. Since each modality of the item could deliver the same characteristics of the item as other modalities, as well as certain exclusive features of the item, overlooking the inter-modal compatibility could yield sub-optimal performance. To address these issues, a multi-modal outfit compatibility modeling scheme with modality-oriented graph learning is proposed, dubbed as MOCM-MGL, which takes both the visual, textual, and category modalities as input and jointly propagates the intra-modal and inter-modal compatibilities among fashion items. Experimental results on the real-world Polyvore Outfits-ND and Polyvore Outfits-D datasets have demonstrated the superiority of our proposed model over existing methods.29:T71a,@ARTICLE{9645182,
  author = {Song, Xuemeng and Fang, Shi-Ting and Chen, Xiaolin and Wei, Yinwei and Zhao, Zhongzhou and Nie, Liqiang},
  journal = {IEEE Transactions on Multimedia},
  title = {Modality-Oriented Graph Learning Toward Outfit Compatibility Modeling},
  year = {2023},
  volume = {25},
  number = {},
  pages = {856--867},
  abstract = {Outfit compatibility modeling, which aims to automatically evaluate the matching degree of an outfit, has drawn great research attention. Regarding the comprehensive evaluation, several previous studies have attempted to solve the task of outfit compatibility modeling by integrating the multi-modal information of fashion items. However, these methods primarily focus on fusing the visual and textual modalities, but seldom consider the category modality as an essential modality. In addition, they mainly focus on the exploration of the intra-modal compatibility relation among fashion items in an outfit but ignore the importance of the inter-modal compatibility relation, i.e., the compatibility across different modalities between fashion items. Since each modality of the item could deliver the same characteristics of the item as other modalities, as well as certain exclusive features of the item, overlooking the inter-modal compatibility could yield sub-optimal performance. To address these issues, a multi-modal outfit compatibility modeling scheme with modality-oriented graph learning is proposed, dubbed as MOCM-MGL, which takes both the visual, textual, and category modalities as input and jointly propagates the intra-modal and inter-modal compatibilities among fashion items. Experimental results on the real-world Polyvore Outfits-ND and Polyvore Outfits-D datasets have demonstrated the superiority of our proposed model over existing methods.}
}2a:T6cd,In this work, we investigate the user identity linkage task across different social media platforms based on heterogeneous multi-modal posts and social connections. This task is non-trivial due to the following two challenges. 1) As each user involves both intra multi-modal posts and inter social connections, how to accurately fulfil the user representation learning from both intra and inter perspectives constitutes the main challenge. And 2) even representations distributed on different platforms of the same identity tend to be distinct (i.e., the semantic gap problem) owing to discrepant data distribution of different platforms. Hence, how to alleviate the semantic gap problem poses another tough challenge. To this end, we propose a novel adversarial-enhanced hybrid graph network (AHG-Net), consisting of three key components: user representation extraction, hybrid user representation learning, and adversarial learning. Specifically, AHG-Net first employs advanced deep learning techniques to extract the user's intermediate representations from his/her heterogeneous multi-modal posts and social connections. Then AHG-Net unifies the intra-user representation learning and inter-user representation learning with a hybrid graph network. Finally, AHG-Net adopts adversarial learning to encourage the learned user presentations of the same identity to be similar using a semantic discriminator. Towards evaluation, we create a multi-modal user identity linkage dataset by augmenting an existing dataset with 62,021 images collected from Twitter and Foursquare. Extensive experiments validate the superiority of the proposed network. Meanwhile, we release the dataset, codes, and parameters to facilitate the research community.2b:T877,@inproceedings{10.1145/3404835.3462946,
  author = {Chen, Xiaolin and Song, Xuemeng and Peng, Guozhen and Feng, Shanshan and Nie, Liqiang},
  title = {Adversarial-Enhanced Hybrid Graph Network for User Identity Linkage},
  year = {2021},
  publisher = {ACM},
  booktitle = {Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {1084-–1093},
  abstract = {In this work, we investigate the user identity linkage task across different social media platforms based on heterogeneous multi-modal posts and social connections. This task is non-trivial due to the following two challenges. 1) As each user involves both intra multi-modal posts and inter social connections, how to accurately fulfil the user representation learning from both intra and inter perspectives constitutes the main challenge. And 2) even representations distributed on different platforms of the same identity tend to be distinct (i.e., the semantic gap problem) owing to discrepant data distribution of different platforms. Hence, how to alleviate the semantic gap problem poses another tough challenge. To this end, we propose a novel adversarial-enhanced hybrid graph network (AHG-Net), consisting of three key components: user representation extraction, hybrid user representation learning, and adversarial learning. Specifically, AHG-Net first employs advanced deep learning techniques to extract the user's intermediate representations from his/her heterogeneous multi-modal posts and social connections. Then AHG-Net unifies the intra-user representation learning and inter-user representation learning with a hybrid graph network. Finally, AHG-Net adopts adversarial learning to encourage the learned user presentations of the same identity to be similar using a semantic discriminator. Towards evaluation, we create a multi-modal user identity linkage dataset by augmenting an existing dataset with 62,021 images collected from Twitter and Foursquare. Extensive experiments validate the superiority of the proposed network. Meanwhile, we release the dataset, codes, and parameters to facilitate the research community.}
}2c:T4fa,In this paper, we work towards linking users’ identities on different social media platforms by exploring the user-generated contents (UGCs). This task is non-trivial due to the following challenges. 1) As UGCs involve multiple modalities (e.g., text and image), how to accurately characterize the user account based on their heterogeneous multi-modal UGCs poses the main challenge. 2) As people tend to post similar UGCs on different social media platforms during the same period, how to effectively model the temporal post correlation is a crucial challenge. And 3) no public benchmark dataset is available to support our user identity linkage based on heterogeneous UGCs with timestamps. Towards this end, we present an attentive time-aware user identity linkage scheme, which seamlessly integrates the temporal post correlation modeling and attentive user similarity modeling. To facilitate the evaluation, we create a comprehensive large-scale user identity linkage dataset from two popular social media platforms: Instagram and Twitter. Extensive experiments have been conducted on our dataset and the results verify the effectiveness of the proposed scheme. As a residual product, we have released the dataset, codes, and parameters to facilitate other researchers.2d:T652,@ARTICLE{9246515,
  author = {Chen, Xiaolin and Song, Xuemeng and Cui, Siwei and Gan, Tian and Cheng, Zhiyong and Nie, Liqiang},
  journal = {IEEE Transactions on Multimedia},
  title = {User Identity Linkage Across Social Media via Attentive Time-Aware User Modeling},
  year = {2021},
  volume = {23},
  pages = {3957-3967},
  abstract = {In this paper, we work towards linking users’ identities on different social media platforms by exploring the user-generated contents (UGCs). This task is non-trivial due to the following challenges. 1) As UGCs involve multiple modalities (e.g., text and image), how to accurately characterize the user account based on their heterogeneous multi-modal UGCs poses the main challenge. 2) As people tend to post similar UGCs on different social media platforms during the same period, how to effectively model the temporal post correlation is a crucial challenge. And 3) no public benchmark dataset is available to support our user identity linkage based on heterogeneous UGCs with timestamps. Towards this end, we present an attentive time-aware user identity linkage scheme, which seamlessly integrates the temporal post correlation modeling and attentive user similarity modeling. To facilitate the evaluation, we create a comprehensive large-scale user identity linkage dataset from two popular social media platforms: Instagram and Twitter. Extensive experiments have been conducted on our dataset and the results verify the effectiveness of the proposed scheme. As a residual product, we have released the dataset, codes, and parameters to facilitate other researchers.}
}2e:T52b,Due to the complex and dynamic environment of social media, user generated contents (UGCs) may inadvertently leak users’ personal aspects, such as the personal attributes, relationships and even the health condition, and thus place users at high privacy risks. Limited research efforts, thus far, have been dedicated to the privacy detection from users’ unstructured data (i.e., UGCs). Moreover, existing efforts mainly focus on applying conventional machine learning techniques directly to traditional hand-crafted privacy-oriented features, ignoring the powerful representing capability of the advanced neural networks. In light of this, in this article, we present a fine-grained privacy detection network (GrHA) equipped with graph-regularized hierarchical attentive representation learning. In particular, the proposed GrHA explores the semantic correlations among personal aspects with graph convolutional networks to enhance the regularization for the UGC representation learning, and, hence, fulfil effective fine-grained privacy detection. Extensive experiments on a real-world dataset demonstrate the superiority of the proposed model over state-of-the-art competitors in terms of eight standard metrics. As a byproduct, we have released the codes and involved parameters to facilitate the research community.2f:T6b6,@article{10.1145/3406109,
  author = {Chen, Xiaolin and Song, Xuemeng and Ren, Ruiyang and Zhu, Lei and Cheng, Zhiyong and Nie, Liqiang},
  title = {Fine-Grained Privacy Detection with Graph-Regularized Hierarchical Attentive Representation Learning},
  year = {2020},
  publisher = {ACM},
  volume = {38},
  number = {4},
  journal = {ACM Transactions on Information Systems},
  abstract = {Due to the complex and dynamic environment of social media, user generated contents (UGCs) may inadvertently leak users’ personal aspects, such as the personal attributes, relationships and even the health condition, and thus place users at high privacy risks. Limited research efforts, thus far, have been dedicated to the privacy detection from users’ unstructured data (i.e., UGCs). Moreover, existing efforts mainly focus on applying conventional machine learning techniques directly to traditional hand-crafted privacy-oriented features, ignoring the powerful representing capability of the advanced neural networks. In light of this, in this article, we present a fine-grained privacy detection network (GrHA) equipped with graph-regularized hierarchical attentive representation learning. In particular, the proposed GrHA explores the semantic correlations among personal aspects with graph convolutional networks to enhance the regularization for the UGC representation learning, and, hence, fulfil effective fine-grained privacy detection. Extensive experiments on a real-world dataset demonstrate the superiority of the proposed model over state-of-the-art competitors in terms of eight standard metrics. As a byproduct, we have released the codes and involved parameters to facilitate the research community.}
}e:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L18",null,{"config":{"type":"publication","title":"Publications","source":"publications.bib"},"publications":[{"id":"chen2025dual","title":"Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haokun Wen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Weili Guan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangyu Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2509.07817","conference":"","abstract":"$19","description":"","selected":true,"preview":"2025_DK2R.jpg","bibtex":"$1a"},{"id":"becattini2024interactive","title":"Interactive Garment Recommendation with User in the Loop","authors":[{"name":"Federico Becattini","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Andrea Puccia","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haokun Wen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Alberto Del Bimbo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"ACM Transactions on Multimedia Computing, Communications, and Applications","conference":"","volume":"21","issue":"1","pages":"37:1--37:21","abstract":"Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.","description":"","selected":false,"preview":"2024_inter.png","bibtex":"$1b"},{"id":"10.1145/3704811","title":"Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jianhui Zuo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tat-Seng Chua","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"ACM Transactions on Multimedia Computing, Communications, and Applications","conference":"","volume":"21","issue":"2","pages":"1--22","code":"https://multimodaldialogs.wixstudio.com/dmdu","abstract":"$1c","description":"","selected":true,"preview":"2924_DMDU.png","bibtex":"$1d"},{"id":"wen2024simple","title":"Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval","authors":[{"name":"Haokun Wen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tat-Seng Chua","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval","pages":"229--239","code":"https://github.com/haokunwen/DQU-CIR","abstract":"$1e","description":"","selected":false,"preview":"2024_CIR.png","bibtex":"$1f"},{"id":"10.1145/3606368","title":"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Jing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shuo Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linmei Hu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems","conference":"","volume":"42","issue":"2","pages":"1--25","code":"https://multimodaldialog.wixsite.com/website","abstract":"$20","description":"","selected":true,"preview":"2023_DKMD.png","bibtex":"$21"},{"id":"10.1145/3539618.3591673","title":"Dual Semantic Knowledge Composed Multimodal Dialog Systems","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tat-Seng Chua","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval","pages":"1--25","code":"https://dialogdk2r.wixsite.com/anonymous7357","abstract":"$22","description":"","selected":true,"preview":"2023_MDS.png","bibtex":"$23"},{"id":"Li2023DoVT","title":"Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR","authors":[{"name":"Zhenyang Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yangyang Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ke-Jyun Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mohan Kankanhalli","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:6:tags","researchArea":"transformer-architectures","journal":"Proceedings of the ACM International Conference on Multimedia","conference":"","abstract":"$24","description":"","selected":false,"preview":"2023_MM.png","bibtex":"$25"},{"id":"10.1609/aaai.v37i8.26138","title":"Mutual-enhanced incongruity learning network for multi-modal sarcasm detection","authors":[{"name":"Yang Qiao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Jing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:7:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the AAAI Conference on Artificial Intelligence","abstract":"$26","description":"","selected":false,"preview":"2023_SCA.png","bibtex":"$27"},{"id":"9645182","title":"Modality-Oriented Graph Learning Toward Outfit Compatibility Modeling","authors":[{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shi-Ting Fang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinwei Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhongzhou Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:8:tags","researchArea":"machine-learning","journal":"IEEE Transactions on Multimedia","conference":"","volume":"25","issue":"","pages":"856--867","code":"https://outfitcompatibility.wixsite.com/mocm-mgl","abstract":"$28","description":"","selected":false,"preview":"2022_Fashion.png","bibtex":"$29"},{"id":"10.1145/3404835.3462946","title":"Adversarial-Enhanced Hybrid Graph Network for User Identity Linkage","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guozhen Peng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shanshan Feng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2021,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:9:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval","pages":"1084-–1093","code":"https://anonymous819.wixsite.com/ahg-net","abstract":"$2a","description":"","selected":false,"preview":"2022_alg.png","bibtex":"$2b"},{"id":"9246515","title":"User Identity Linkage Across Social Media via Attentive Time-Aware User Modeling","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Siwei Cui","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tian Gan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyong Cheng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2021,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:10:tags","researchArea":"machine-learning","journal":"IEEE Transactions on Multimedia","conference":"","volume":"23","pages":"3957-3967","code":"https://dialogdk2r.wixsite.com/usernet","abstract":"$2c","description":"","selected":false,"preview":"2020_TMM.png","bibtex":"$2d"},{"id":"10.1145/3406109","title":"Fine-Grained Privacy Detection with Graph-Regularized Hierarchical Attentive Representation Learning","authors":[{"name":"Xiaolin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuemeng Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyang Ren","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyong Cheng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liqiang Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2020,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:11:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems","conference":"","volume":"38","issue":"4","code":"https://github.com/Fine-grainedPrivacyDetection/GrHA","abstract":"$2e","description":"","selected":false,"preview":"2020_TOIS.png","bibtex":"$2f"}]}],false,false]}]
14:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:null
30:I[622,[],"IconMark"]
12:{"metadata":[["$","title","0",{"children":"Publications | Xiaolin Chen"}],["$","meta","1",{"name":"author","content":"Xiaolin Chen"}],["$","meta","2",{"name":"keywords","content":"Xiaolin Chen,PhD,Research,National University of Singapore (NUS)"}],["$","meta","3",{"name":"creator","content":"Xiaolin Chen"}],["$","meta","4",{"name":"publisher","content":"Xiaolin Chen"}],["$","meta","5",{"property":"og:title","content":"Xiaolin Chen"}],["$","meta","6",{"property":"og:description","content":"Homepage of Xiaolin"}],["$","meta","7",{"property":"og:site_name","content":"Xiaolin Chen's Academic Website"}],["$","meta","8",{"property":"og:locale","content":"en_US"}],["$","meta","9",{"property":"og:type","content":"website"}],["$","meta","10",{"name":"twitter:card","content":"summary"}],["$","meta","11",{"name":"twitter:title","content":"Xiaolin Chen"}],["$","meta","12",{"name":"twitter:description","content":"Homepage of Xiaolin"}],["$","link","13",{"rel":"icon","href":"/favicon.svg"}],["$","$L30","14",{}]],"error":null,"digest":"$undefined"}
17:"$12:metadata"
